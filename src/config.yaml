LLAMA32_CONFIG:
  vocab_size: 128256
  context_length: 131072
  embedding_dim: 2048
  n_heads: 32
  n_layers: 16
  hidden_dim: 8192
  n_kv_groups: 8
  rope_base: 500000.0
  dtype: bfloat16
  param_dtype: bfloat16
  rope_freq:
    factor: 32.0
    low_freq_factor: 1.0
    high_freq_factor: 4.0
    original_context_length: 8192